{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcda065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crypter70/.pyenv/versions/fine_tuning_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "import torch\n",
    "\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# MPS checking\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac29aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79454760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the model by change this variable\n",
    "MODEL_CHECKPOINT = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae5f59d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 22021\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"vhtran/en-id\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48774234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'id': 'Philip Johnson, seorang professor di University of California di Barkeley yang juga salah seorang pengkritik Darwinisme terkemuka, menggambarkan pertentangan antara kenyataan paleontologis ini dengan Darwinisme.',\n",
       "  'en': \"Phillip Johnson, a professor at the University of California at Berkeley who is also one of the world's foremost critics of Darwinism, describes the contradiction between this paleontological truth and Darwinism.\"}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "496ab175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'id': Value(dtype='string', id=None),\n",
       "  'en': Value(dtype='string', id=None)}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d31bf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4d43487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': 'Bank umum, lanjutnya juga bisa memanfaatkan kantor cabangnya untuk membuat 'office chanelling' bank syariah.', 'en': 'Commercial banks, she added, can also use their branch offices to build a bank syariah channeling office.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': 'nurut UK Trade and Investment UKTI, nilai ekspor Indonesia ke Inggris sepanjang tahun 2005 tercatat senilai 517 juta Pound Sterling , yang kebanyakan didominasi jenis barang-barang tekstil dan alas kaki.', 'en': 'According to UK Investment and Trade UKIT data, Indonesia's exports to Britain in 2005 stood at 912 million pound sterling against Britain's total imports of 912 million pound sterling, most of which were textile products and footwear.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': 'Hubungan Grant dan Hurley yang telah terjalin lama berakhir pada 2000.', 'en': 'Grant's long relationship with Hurley ended in 2000.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'id': 'Di ibukota negeri itu, Conakry, jumlah tentara dan polisi yang berada di jalan berkurang dibandingkan jumlah mereka Senin, hari paling berdarah dalam pemogokan tersebut, ketika pasukan keamanan melepaskan tembakan ke arah demonstran selama bentrokan.', 'en': 'In the capital Conakry, there were fewer police and troops on the streets  than on Monday, the bloodiest day of the strike, when security forces fired at demonstrators during running battles.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'id': 'Isu ketidakamanan data e-mail kita sempat dipersoalkan karena data kita ternyata bisa dibaca oleh server Google untuk mendistribusikan iklan.', 'en': 'The issue on inconvenience related to the content of our emails was on one occasion discussed, as apparently our data could be read by Google server for distribution of their advertisements'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e176913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"id\"\n",
    "target_lang = \"en\"\n",
    "prefix = \"translate Indonesia to English: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b3741a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/22021 [00:00<?, ? examples/s]/Users/crypter70/.pyenv/versions/fine_tuning_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 22021/22021 [00:01<00:00, 11800.07 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 9492.79 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 11020.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_books = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47adf895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab183c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_books[\"train\"],\n",
    "    eval_dataset=tokenized_books[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45290f11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine_tuning_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
